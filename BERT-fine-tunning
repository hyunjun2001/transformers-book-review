{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 파이토치 트랜스포머를 활용한 자연어 처리와 컴퓨터 비전 심층학습 복습용 노트입니다.","metadata":{}},{"cell_type":"code","source":"%config Completer.use_jedi = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T07:10:39.286304Z","iopub.execute_input":"2024-11-21T07:10:39.286554Z","iopub.status.idle":"2024-11-21T07:10:39.297480Z","shell.execute_reply.started":"2024-11-21T07:10:39.286530Z","shell.execute_reply":"2024-11-21T07:10:39.296649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install Korpora","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T07:10:39.299525Z","iopub.execute_input":"2024-11-21T07:10:39.299922Z","iopub.status.idle":"2024-11-21T07:10:49.188365Z","shell.execute_reply.started":"2024-11-21T07:10:39.299882Z","shell.execute_reply":"2024-11-21T07:10:49.187309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 네이버 영화 리뷰 데이터 불러오기\n\nimport numpy as np\nimport pandas as pd\nfrom Korpora import Korpora\n\ncorpus = Korpora.load(\"nsmc\")\ndf = pd.DataFrame(corpus.test).sample(20000, random_state=42)\ntrain, valid, test = np.split(\n    df.sample(frac=1, random_state=42), [int(0.6 * len(df)), int(0.8 * len(df))]\n)\n\nprint(train.head(5).to_markdown())\nprint(len(train))\nprint(len(test))\nprint(len(valid))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T07:10:49.190139Z","iopub.execute_input":"2024-11-21T07:10:49.190429Z","iopub.status.idle":"2024-11-21T07:10:52.038887Z","shell.execute_reply.started":"2024-11-21T07:10:49.190400Z","shell.execute_reply":"2024-11-21T07:10:52.038041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# BERT의 BertTokenizer 클래스로 데이터를 전처리\n\nimport torch\nfrom transformers import BertTokenizer\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.utils.data import RandomSampler, SequentialSampler\n\ndef make_dataset(data, tokenizer, device):\n    tokenized = tokenizer(\n        text = data.text.tolist(),\n        padding=\"longest\",\n        truncation=True,\n        return_tensors=\"pt\"\n    )\n    input_ids = tokenized[\"input_ids\"].to(device)\n    attention_mask = tokenized[\"attention_mask\"].to(device)\n    labels = torch.tensor(data.label.values, dtype=torch.long).to(device)\n    return TensorDataset(input_ids, attention_mask, labels)\n\ndef get_dataloader(dataset, sampler, batch_size):\n    data_sampler = sampler(dataset)\n    dataloader = DataLoader(dataset, sampler=data_sampler, batch_size=batch_size)\n    return dataloader\n\nepochs = 5\nbatch_size = 32\ndevice = \"cuda\" if torch.cuda.is_available else \"cpu\"\ntokenizer = BertTokenizer.from_pretrained(\n    pretrained_model_name_or_path = \"bert-base-multilingual-cased\",\n    do_lower_case = False\n)\n\ntrain_dataset = make_dataset(train,tokenizer, device)\ntrain_dataloader = get_dataloader(train_dataset, RandomSampler, batch_size)\n\nvalid_dataset = make_dataset(valid, tokenizer, device)\nvalid_dataloader = get_dataloader(valid_dataset, SequentialSampler, batch_size)\n\ntest_dataset = make_dataset(test, tokenizer, device)\ntest_dataloader = get_dataloader(test_dataset, SequentialSampler, batch_size)\n\nprint(train_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T07:10:52.040003Z","iopub.execute_input":"2024-11-21T07:10:52.040356Z","iopub.status.idle":"2024-11-21T07:11:03.468849Z","shell.execute_reply.started":"2024-11-21T07:10:52.040330Z","shell.execute_reply":"2024-11-21T07:11:03.467994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# BERT 모델 선언\n\nfrom torch import optim\nfrom transformers import BertForSequenceClassification\n\nmodel = BertForSequenceClassification.from_pretrained(\n    pretrained_model_name_or_path = \"bert-base-multilingual-cased\",\n    num_labels=2\n).to(device)\n\noptimizer = optim.AdamW(model.parameters(), lr=1e-5, eps=1e-8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T07:11:03.470761Z","iopub.execute_input":"2024-11-21T07:11:03.471214Z","iopub.status.idle":"2024-11-21T07:11:08.100660Z","shell.execute_reply.started":"2024-11-21T07:11:03.471185Z","shell.execute_reply":"2024-11-21T07:11:08.099956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for main_name, main_module in model.named_children():\n    print(main_name)\n    for sub_name, sub_module in main_module.named_children():\n        print(\"ㄴ\",sub_name)\n        for ssub_name, ssub_module in sub_module.named_children():\n            print(\"| ㄴ\",ssub_name)\n            for sssub_name, sssub_module in ssub_module.named_children():\n                print(\"| | ㄴ\", sssub_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T07:11:08.101595Z","iopub.execute_input":"2024-11-21T07:11:08.101955Z","iopub.status.idle":"2024-11-21T07:11:08.107589Z","shell.execute_reply.started":"2024-11-21T07:11:08.101930Z","shell.execute_reply":"2024-11-21T07:11:08.106677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# BERT 모델 학습 및 검증\n\nimport numpy as np\nfrom torch import nn\n\ndef calc_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\ndef train(model, optimizer, dataloader):\n    model.train()\n    train_loss = 0.0\n\n    for input_ids, attention_mask, labels in dataloader:\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n\n        loss = outputs.loss\n        train_loss += loss\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    train_loss = train_loss / len(dataloader)\n    return train_loss\n        \n\ndef evaluation(model, dataloader):\n    with torch.no_grad():\n        model.eval()\n        criterion = nn.CrossEntropyLoss()\n        val_loss, val_accuracy = 0.0, 0.0\n\n        for input_ids, attention_mask, labels in dataloader:\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            logits = outputs.logits\n\n            loss = criterion(logits, labels)\n            logits = logits.detach().cpu().numpy()\n            label_ids = labels.to(\"cpu\").numpy()\n            accuracy = calc_accuracy(logits, label_ids)\n\n            val_loss += loss\n            val_accuracy += accuracy\n\n    val_loss = val_loss/len(dataloader)\n    val_accuracy = val_accuracy/len(dataloader)\n    return val_loss, val_accuracy\n\nbest_loss = 1000\nfor epoch in range(epochs):\n    train_loss = train(model, optimizer, train_dataloader)\n    val_loss, val_accuracy = evaluation(model, valid_dataloader)\n    print(f\"epoch {epoch+1},train loss: {train_loss}, valid loss, accuracy{ val_loss}, {val_accuracy}\")\n\n    if val_loss < best_loss:\n        val_loss = best_loss\n        torch.save(model.state_dict(), \"/kaggle/working/BertForSequenceClassification.pt\")\n        print(\"save 완\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T07:11:08.109120Z","iopub.execute_input":"2024-11-21T07:11:08.109447Z","iopub.status.idle":"2024-11-21T07:23:37.255115Z","shell.execute_reply.started":"2024-11-21T07:11:08.109411Z","shell.execute_reply":"2024-11-21T07:23:37.254002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(\n    pretrained_model_name_or_path = \"bert-base-multilingual-cased\",\n    num_labels = 2\n).to(device)\n\nmodel.load_state_dict(torch.load(\"/kaggle/working/BertForSequenceClassification.pt\"))\n\ntest_loss, test_accuracy = evaluation(model, test_dataloader)\nprint(f\"test loss : {test_loss:.4f}\")\nprint(f\"test accuracy : {test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T07:23:37.256516Z","iopub.execute_input":"2024-11-21T07:23:37.256795Z","iopub.status.idle":"2024-11-21T07:23:50.535737Z","shell.execute_reply.started":"2024-11-21T07:23:37.256770Z","shell.execute_reply":"2024-11-21T07:23:50.534873Z"}},"outputs":[],"execution_count":null}]}