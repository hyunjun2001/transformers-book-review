{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 파이토치 트랜스포머를 활용한 자연어 처리와 컴퓨터 비전 심층학습 복습용 노트입니다.","metadata":{}},{"cell_type":"code","source":"%config Completer.use_jedi = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T07:10:39.286304Z","iopub.execute_input":"2024-11-21T07:10:39.286554Z","iopub.status.idle":"2024-11-21T07:10:39.297480Z","shell.execute_reply.started":"2024-11-21T07:10:39.286530Z","shell.execute_reply":"2024-11-21T07:10:39.296649Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install Korpora","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T07:10:39.299525Z","iopub.execute_input":"2024-11-21T07:10:39.299922Z","iopub.status.idle":"2024-11-21T07:10:49.188365Z","shell.execute_reply.started":"2024-11-21T07:10:39.299882Z","shell.execute_reply":"2024-11-21T07:10:49.187309Z"}},"outputs":[{"name":"stdout","text":"Collecting Korpora\n  Downloading Korpora-0.2.0-py3-none-any.whl.metadata (26 kB)\nCollecting dataclasses>=0.6 (from Korpora)\n  Downloading dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from Korpora) (1.26.4)\nRequirement already satisfied: tqdm>=4.46.0 in /opt/conda/lib/python3.10/site-packages (from Korpora) (4.66.4)\nRequirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.10/site-packages (from Korpora) (2.32.3)\nCollecting xlrd>=1.2.0 (from Korpora)\n  Downloading xlrd-2.0.1-py2.py3-none-any.whl.metadata (3.4 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->Korpora) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->Korpora) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->Korpora) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->Korpora) (2024.8.30)\nDownloading Korpora-0.2.0-py3-none-any.whl (57 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dataclasses-0.6-py3-none-any.whl (14 kB)\nDownloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: dataclasses, xlrd, Korpora\nSuccessfully installed Korpora-0.2.0 dataclasses-0.6 xlrd-2.0.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 네이버 영화 리뷰 데이터 불러오기\n\nimport numpy as np\nimport pandas as pd\nfrom Korpora import Korpora\n\ncorpus = Korpora.load(\"nsmc\")\ndf = pd.DataFrame(corpus.test).sample(20000, random_state=42)\ntrain, valid, test = np.split(\n    df.sample(frac=1, random_state=42), [int(0.6 * len(df)), int(0.8 * len(df))]\n)\n\nprint(train.head(5).to_markdown())\nprint(len(train))\nprint(len(test))\nprint(len(valid))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T07:10:49.190139Z","iopub.execute_input":"2024-11-21T07:10:49.190429Z","iopub.status.idle":"2024-11-21T07:10:52.038887Z","shell.execute_reply.started":"2024-11-21T07:10:49.190400Z","shell.execute_reply":"2024-11-21T07:10:52.038041Z"}},"outputs":[{"name":"stdout","text":"\n    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n\n    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n\n    # Description\n    Author : e9t@github\n    Repository : https://github.com/e9t/nsmc\n    References : www.lucypark.kr/docs/2015-pyconkr/#39\n\n    Naver sentiment movie corpus v1.0\n    This is a movie review dataset in the Korean language.\n    Reviews were scraped from Naver Movies.\n\n    The dataset construction is based on the method noted in\n    [Large movie review dataset][^1] from Maas et al., 2011.\n\n    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n\n    # License\n    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n    Details in https://creativecommons.org/publicdomain/zero/1.0/\n\n","output_type":"stream"},{"name":"stderr","text":"[nsmc] download ratings_train.txt: 14.6MB [00:00, 187MB/s]\n[nsmc] download ratings_test.txt: 4.90MB [00:00, 72.5MB/s]\n","output_type":"stream"},{"name":"stdout","text":"|       | text                                                     |   label |\n|------:|:---------------------------------------------------------|--------:|\n| 26891 | 역시 코믹액션은 성룡, 홍금보, 원표 삼인방이 최고지!!     |       1 |\n| 25024 | 점수 후하게 줘야것네 별 반개~                            |       0 |\n| 11666 | 오랜만에 느낄수 있는 [감독] 구타욕구.                    |       0 |\n| 40303 | 본지는 좀 됬지만 극장서 돈주고 본게 아직까지 아까운 영화 |       0 |\n| 18010 | 징키스칸이란 소재를 가지고 이것밖에 못만드냐             |       0 |\n12000\n4000\n4000\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n  return bound(*args, **kwds)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# BERT의 BertTokenizer 클래스로 데이터를 전처리\n\nimport torch\nfrom transformers import BertTokenizer\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.utils.data import RandomSampler, SequentialSampler\n\ndef make_dataset(data, tokenizer, device):\n    tokenized = tokenizer(\n        text = data.text.tolist(),\n        padding=\"longest\",\n        truncation=True,\n        return_tensors=\"pt\"\n    )\n    input_ids = tokenized[\"input_ids\"].to(device)\n    attention_mask = tokenized[\"attention_mask\"].to(device)\n    labels = torch.tensor(data.label.values, dtype=torch.long).to(device)\n    return TensorDataset(input_ids, attention_mask, labels)\n\ndef get_dataloader(dataset, sampler, batch_size):\n    data_sampler = sampler(dataset)\n    dataloader = DataLoader(dataset, sampler=data_sampler, batch_size=batch_size)\n    return dataloader\n\nepochs = 5\nbatch_size = 32\ndevice = \"cuda\" if torch.cuda.is_available else \"cpu\"\ntokenizer = BertTokenizer.from_pretrained(\n    pretrained_model_name_or_path = \"bert-base-multilingual-cased\",\n    do_lower_case = False\n)\n\ntrain_dataset = make_dataset(train,tokenizer, device)\ntrain_dataloader = get_dataloader(train_dataset, RandomSampler, batch_size)\n\nvalid_dataset = make_dataset(valid, tokenizer, device)\nvalid_dataloader = get_dataloader(valid_dataset, SequentialSampler, batch_size)\n\ntest_dataset = make_dataset(test, tokenizer, device)\ntest_dataloader = get_dataloader(test_dataset, SequentialSampler, batch_size)\n\nprint(train_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T07:10:52.040003Z","iopub.execute_input":"2024-11-21T07:10:52.040356Z","iopub.status.idle":"2024-11-21T07:11:03.468849Z","shell.execute_reply.started":"2024-11-21T07:10:52.040330Z","shell.execute_reply":"2024-11-21T07:11:03.467994Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f72180cf0458421d8c7e77904d92fa73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92ac7f6ef0df4919a604b2e7104e46c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbe9d5c4e38445cca514b9f8794fd430"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44e3b5f8a41f4c748320fc685e6e5ff3"}},"metadata":{}},{"name":"stdout","text":"(tensor([   101,  58466,   9812, 118956, 119122,  59095,  10892,   9434, 118888,\n           117,   9992,  40032,  30005,    117,   9612,  37824,   9410,  12030,\n         42337,  10739,  83491,  12508,    106,    106,    102,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0],\n       device='cuda:0'), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0], device='cuda:0'), tensor(1, device='cuda:0'))\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# BERT 모델 선언\n\nfrom torch import optim\nfrom transformers import BertForSequenceClassification\n\nmodel = BertForSequenceClassification.from_pretrained(\n    pretrained_model_name_or_path = \"bert-base-multilingual-cased\",\n    num_labels=2\n).to(device)\n\noptimizer = optim.AdamW(model.parameters(), lr=1e-5, eps=1e-8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T07:11:03.470761Z","iopub.execute_input":"2024-11-21T07:11:03.471214Z","iopub.status.idle":"2024-11-21T07:11:08.100660Z","shell.execute_reply.started":"2024-11-21T07:11:03.471185Z","shell.execute_reply":"2024-11-21T07:11:08.099956Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0390ed00956f4d6ea271edad22acaed7"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"for main_name, main_module in model.named_children():\n    print(main_name)\n    for sub_name, sub_module in main_module.named_children():\n        print(\"ㄴ\",sub_name)\n        for ssub_name, ssub_module in sub_module.named_children():\n            print(\"| ㄴ\",ssub_name)\n            for sssub_name, sssub_module in ssub_module.named_children():\n                print(\"| | ㄴ\", sssub_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T07:11:08.101595Z","iopub.execute_input":"2024-11-21T07:11:08.101955Z","iopub.status.idle":"2024-11-21T07:11:08.107589Z","shell.execute_reply.started":"2024-11-21T07:11:08.101930Z","shell.execute_reply":"2024-11-21T07:11:08.106677Z"}},"outputs":[{"name":"stdout","text":"bert\nㄴ embeddings\n| ㄴ word_embeddings\n| ㄴ position_embeddings\n| ㄴ token_type_embeddings\n| ㄴ LayerNorm\n| ㄴ dropout\nㄴ encoder\n| ㄴ layer\n| | ㄴ 0\n| | ㄴ 1\n| | ㄴ 2\n| | ㄴ 3\n| | ㄴ 4\n| | ㄴ 5\n| | ㄴ 6\n| | ㄴ 7\n| | ㄴ 8\n| | ㄴ 9\n| | ㄴ 10\n| | ㄴ 11\nㄴ pooler\n| ㄴ dense\n| ㄴ activation\ndropout\nclassifier\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# BERT 모델 학습 및 검증\n\nimport numpy as np\nfrom torch import nn\n\ndef calc_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\ndef train(model, optimizer, dataloader):\n    model.train()\n    train_loss = 0.0\n\n    for input_ids, attention_mask, labels in dataloader:\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n\n        loss = outputs.loss\n        train_loss += loss\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    train_loss = train_loss / len(dataloader)\n    return train_loss\n        \n\ndef evaluation(model, dataloader):\n    with torch.no_grad():\n        model.eval()\n        criterion = nn.CrossEntropyLoss()\n        val_loss, val_accuracy = 0.0, 0.0\n\n        for input_ids, attention_mask, labels in dataloader:\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            logits = outputs.logits\n\n            loss = criterion(logits, labels)\n            logits = logits.detach().cpu().numpy()\n            label_ids = labels.to(\"cpu\").numpy()\n            accuracy = calc_accuracy(logits, label_ids)\n\n            val_loss += loss\n            val_accuracy += accuracy\n\n    val_loss = val_loss/len(dataloader)\n    val_accuracy = val_accuracy/len(dataloader)\n    return val_loss, val_accuracy\n\nbest_loss = 1000\nfor epoch in range(epochs):\n    train_loss = train(model, optimizer, train_dataloader)\n    val_loss, val_accuracy = evaluation(model, valid_dataloader)\n    print(f\"epoch {epoch+1},train loss: {train_loss}, valid loss, accuracy{ val_loss}, {val_accuracy}\")\n\n    if val_loss < best_loss:\n        val_loss = best_loss\n        torch.save(model.state_dict(), \"/kaggle/working/BertForSequenceClassification.pt\")\n        print(\"save 완\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T07:11:08.109120Z","iopub.execute_input":"2024-11-21T07:11:08.109447Z","iopub.status.idle":"2024-11-21T07:23:37.255115Z","shell.execute_reply.started":"2024-11-21T07:11:08.109411Z","shell.execute_reply":"2024-11-21T07:23:37.254002Z"}},"outputs":[{"name":"stdout","text":"epoch 1,train loss: 0.5487060546875, valid loss, accuracy0.4363084137439728, 0.78925\nsave 완\nepoch 2,train loss: 0.4042300283908844, valid loss, accuracy0.40038713812828064, 0.81225\nsave 완\nepoch 3,train loss: 0.3149094879627228, valid loss, accuracy0.4312754273414612, 0.8155\nsave 완\nepoch 4,train loss: 0.2560823857784271, valid loss, accuracy0.4466226398944855, 0.81875\nsave 완\nepoch 5,train loss: 0.1970904916524887, valid loss, accuracy0.4981912672519684, 0.8255\nsave 완\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(\n    pretrained_model_name_or_path = \"bert-base-multilingual-cased\",\n    num_labels = 2\n).to(device)\n\nmodel.load_state_dict(torch.load(\"/kaggle/working/BertForSequenceClassification.pt\"))\n\ntest_loss, test_accuracy = evaluation(model, test_dataloader)\nprint(f\"test loss : {test_loss:.4f}\")\nprint(f\"test accuracy : {test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T07:23:37.256516Z","iopub.execute_input":"2024-11-21T07:23:37.256795Z","iopub.status.idle":"2024-11-21T07:23:50.535737Z","shell.execute_reply.started":"2024-11-21T07:23:37.256770Z","shell.execute_reply":"2024-11-21T07:23:50.534873Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_30/2858843683.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"/kaggle/working/BertForSequenceClassification.pt\"))\n","output_type":"stream"},{"name":"stdout","text":"test loss : 0.5251\ntest accuracy : 0.8133\n","output_type":"stream"}],"execution_count":8}]}