{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%config Completer.use_jedi = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T06:44:45.286909Z","iopub.execute_input":"2024-12-05T06:44:45.287364Z","iopub.status.idle":"2024-12-05T06:44:45.293749Z","shell.execute_reply.started":"2024-12-05T06:44:45.287331Z","shell.execute_reply":"2024-12-05T06:44:45.292855Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# ELECTRA 모델을 활용한 문장 요약 모델 미세 조정","metadata":{}},{"cell_type":"code","source":"!pip install Korpora","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T06:46:13.205826Z","iopub.execute_input":"2024-12-05T06:46:13.206893Z","iopub.status.idle":"2024-12-05T06:46:23.685709Z","shell.execute_reply.started":"2024-12-05T06:46:13.206841Z","shell.execute_reply":"2024-12-05T06:46:23.684696Z"}},"outputs":[{"name":"stdout","text":"Collecting Korpora\n  Downloading Korpora-0.2.0-py3-none-any.whl.metadata (26 kB)\nCollecting dataclasses>=0.6 (from Korpora)\n  Downloading dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from Korpora) (1.26.4)\nRequirement already satisfied: tqdm>=4.46.0 in /opt/conda/lib/python3.10/site-packages (from Korpora) (4.66.4)\nRequirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.10/site-packages (from Korpora) (2.32.3)\nCollecting xlrd>=1.2.0 (from Korpora)\n  Downloading xlrd-2.0.1-py2.py3-none-any.whl.metadata (3.4 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->Korpora) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->Korpora) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->Korpora) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->Korpora) (2024.6.2)\nDownloading Korpora-0.2.0-py3-none-any.whl (57 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dataclasses-0.6-py3-none-any.whl (14 kB)\nDownloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: dataclasses, xlrd, Korpora\nSuccessfully installed Korpora-0.2.0 dataclasses-0.6 xlrd-2.0.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom Korpora import Korpora\n\n\ncorpus = Korpora.load(\"nsmc\")\ndf = pd.DataFrame(corpus.test).sample(20000, random_state=42)\ntrain, valid, test = np.split(\n    df.sample(frac=1, random_state=42), [int(0.6 * len(df)), int(0.8 * len(df))]\n)\n\nprint(train.head(5).to_markdown())\nprint(f\"train data size : {len(train)}\")\nprint(f\"valid data size : {len(valid)}\")\nprint(f\"test data size : {len(test)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T07:27:09.287377Z","iopub.execute_input":"2024-12-05T07:27:09.287756Z","iopub.status.idle":"2024-12-05T07:27:11.151240Z","shell.execute_reply.started":"2024-12-05T07:27:09.287724Z","shell.execute_reply":"2024-12-05T07:27:11.150362Z"}},"outputs":[{"name":"stdout","text":"\n    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n\n    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n\n    # Description\n    Author : e9t@github\n    Repository : https://github.com/e9t/nsmc\n    References : www.lucypark.kr/docs/2015-pyconkr/#39\n\n    Naver sentiment movie corpus v1.0\n    This is a movie review dataset in the Korean language.\n    Reviews were scraped from Naver Movies.\n\n    The dataset construction is based on the method noted in\n    [Large movie review dataset][^1] from Maas et al., 2011.\n\n    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n\n    # License\n    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n    Details in https://creativecommons.org/publicdomain/zero/1.0/\n\n[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_train.txt\n[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_test.txt\n|       | text                                                     |   label |\n|------:|:---------------------------------------------------------|--------:|\n| 26891 | 역시 코믹액션은 성룡, 홍금보, 원표 삼인방이 최고지!!     |       1 |\n| 25024 | 점수 후하게 줘야것네 별 반개~                            |       0 |\n| 11666 | 오랜만에 느낄수 있는 [감독] 구타욕구.                    |       0 |\n| 40303 | 본지는 좀 됬지만 극장서 돈주고 본게 아직까지 아까운 영화 |       0 |\n| 18010 | 징키스칸이란 소재를 가지고 이것밖에 못만드냐             |       0 |\ntrain data size : 12000\nvalid data size : 4000\ntest data size : 4000\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n  return bound(*args, **kwds)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import torch\nfrom transformers import ElectraTokenizer\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.utils.data import RandomSampler, SequentialSampler\n\n\ndef make_dataset(data, tokenizer, device):\n    tokenized = tokenizer(\n        text=data.text.tolist(),\n        padding=\"longest\",\n        truncation=True,\n        return_tensors=\"pt\"\n    )\n    input_ids = tokenized[\"input_ids\"].to(device)\n    attention_mask = tokenized[\"attention_mask\"].to(device)\n    labels = torch.tensor(data.label.values, dtype=torch.long).to(device)\n    return TensorDataset(input_ids, attention_mask, labels)\n\n\ndef get_datalodader(dataset, sampler, batch_size):\n    data_sampler = sampler(dataset)\n    dataloader = DataLoader(dataset, sampler=data_sampler, batch_size=batch_size)\n    return dataloader\n\n\nepochs = 5\nbatch_size = 32\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntokenizer = ElectraTokenizer.from_pretrained(\n    pretrained_model_name_or_path=\"monologg/koelectra-base-v3-discriminator\",\n    do_lower_case=False,\n)\n\ntrain_dataset = make_dataset(train, tokenizer, device)\ntrain_dataloader = get_datalodader(train_dataset, RandomSampler, batch_size)\n\nvalid_dataset = make_dataset(valid, tokenizer, device)\nvalid_dataloader = get_datalodader(valid_dataset, SequentialSampler, batch_size)\n\ntest_dataset = make_dataset(test, tokenizer, device)\ntest_dataloader = get_datalodader(test_dataset, SequentialSampler, batch_size)\n\nprint(train_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T07:27:13.398379Z","iopub.execute_input":"2024-12-05T07:27:13.398962Z","iopub.status.idle":"2024-12-05T07:27:18.611636Z","shell.execute_reply.started":"2024-12-05T07:27:13.398929Z","shell.execute_reply":"2024-12-05T07:27:18.610701Z"}},"outputs":[{"name":"stdout","text":"(tensor([    2,  6511, 14347,  4087,  4665,  4112,  2924,  4806,    16,  3809,\n         4309,  4275,    16,  3201,  4376,  2891,  4139,  4212,  4007,  6557,\n         4200,     5,     5,     3,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0], device='cuda:0'), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0], device='cuda:0'), tensor(1, device='cuda:0'))\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from torch import optim\nfrom transformers import ElectraForSequenceClassification\n\nmodel = ElectraForSequenceClassification.from_pretrained(\n    pretrained_model_name_or_path= \"monologg/koelectra-base-v3-discriminator\",\n    num_labels=2\n).to(device)\noptimizer = optim.AdamW(model.parameters(), lr=1e-5, eps=1e-8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T07:27:20.735007Z","iopub.execute_input":"2024-12-05T07:27:20.735851Z","iopub.status.idle":"2024-12-05T07:27:21.916396Z","shell.execute_reply.started":"2024-12-05T07:27:20.735815Z","shell.execute_reply":"2024-12-05T07:27:21.915717Z"}},"outputs":[{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"for main_name, main_module in model.named_children():\n    print(main_name)\n    for sub_name, sub_module in main_module.named_children():\n        print(\"ㄴ\", sub_name)\n        for ssub_name, ssub_module in sub_module.named_children():\n            print(\"│  ㄴ\", ssub_name)\n            for sssub_name, sssub_module in ssub_module.named_children():\n                print(\"│  │  ㄴ\", sssub_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T07:27:21.917986Z","iopub.execute_input":"2024-12-05T07:27:21.918767Z","iopub.status.idle":"2024-12-05T07:27:21.925940Z","shell.execute_reply.started":"2024-12-05T07:27:21.918729Z","shell.execute_reply":"2024-12-05T07:27:21.925142Z"}},"outputs":[{"name":"stdout","text":"electra\nㄴ embeddings\n│  ㄴ word_embeddings\n│  ㄴ position_embeddings\n│  ㄴ token_type_embeddings\n│  ㄴ LayerNorm\n│  ㄴ dropout\nㄴ encoder\n│  ㄴ layer\n│  │  ㄴ 0\n│  │  ㄴ 1\n│  │  ㄴ 2\n│  │  ㄴ 3\n│  │  ㄴ 4\n│  │  ㄴ 5\n│  │  ㄴ 6\n│  │  ㄴ 7\n│  │  ㄴ 8\n│  │  ㄴ 9\n│  │  ㄴ 10\n│  │  ㄴ 11\nclassifier\nㄴ dense\nㄴ activation\nㄴ dropout\nㄴ out_proj\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import numpy as np\nfrom torch import nn\n\ndef calc_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\ndef train(model, optimizer, dataloader):\n    model.train()\n    train_loss = 0.0\n\n    for input_ids, attention_mask, labels in dataloader:\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n\n        loss = outputs.loss\n        train_loss += loss.item()\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    train_loss = train_loss / len(dataloader)\n    return train_loss\n\ndef evaluation(model, dataloader):\n    with torch.no_grad():\n        model.eval()\n        criterion = nn.CrossEntropyLoss()\n        val_loss, val_accuracy = 0.0, 0.0\n        \n        for input_ids, attention_mask, labels in dataloader:\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            logits = outputs.logits\n\n            loss = criterion(logits, labels)\n            logits = logits.detach().cpu().numpy()\n            label_ids = labels.to(\"cpu\").numpy()\n            accuracy = calc_accuracy(logits, label_ids)\n            \n            val_loss += loss\n            val_accuracy += accuracy\n    \n    val_loss = val_loss/len(dataloader)\n    val_accuracy = val_accuracy/len(dataloader)\n    return val_loss, val_accuracy\n\n\nbest_loss = 10000\nfor epoch in range(epochs):\n    train_loss = train(model, optimizer, train_dataloader)\n    val_loss, val_accuracy = evaluation(model, valid_dataloader)\n    print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Val Accuracy {val_accuracy:.4f}\")\n\n    if val_loss < best_loss:\n        best_loss = val_loss\n        torch.save(model.state_dict(), \"/kaggle/working/ElectraForSequenceClassification.pt\")\n        print(\"saved\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T07:27:22.450371Z","iopub.execute_input":"2024-12-05T07:27:22.451190Z","iopub.status.idle":"2024-12-05T07:39:21.048280Z","shell.execute_reply.started":"2024-12-05T07:27:22.451153Z","shell.execute_reply":"2024-12-05T07:39:21.047372Z"}},"outputs":[{"name":"stdout","text":"Epoch 1: Train Loss: 0.4525 Val Loss: 0.3304 Val Accuracy 0.8668\nsaved\nEpoch 2: Train Loss: 0.2867 Val Loss: 0.3139 Val Accuracy 0.8770\nsaved\nEpoch 3: Train Loss: 0.2220 Val Loss: 0.3226 Val Accuracy 0.8810\nEpoch 4: Train Loss: 0.1670 Val Loss: 0.3411 Val Accuracy 0.8815\nEpoch 5: Train Loss: 0.1201 Val Loss: 0.4113 Val Accuracy 0.8762\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"model = ElectraForSequenceClassification.from_pretrained(\n    pretrained_model_name_or_path=\"monologg/koelectra-base-v3-discriminator\",\n    num_labels=2\n).to(device)\nmodel.load_state_dict(torch.load(\"/kaggle/working/ElectraForSequenceClassification.pt\"))\n\ntest_loss, test_accuracy = evaluation(model, test_dataloader)\nprint(test_loss)\nprint(test_accuracy)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T07:39:53.795439Z","iopub.execute_input":"2024-12-05T07:39:53.796159Z","iopub.status.idle":"2024-12-05T07:40:08.638180Z","shell.execute_reply.started":"2024-12-05T07:39:53.796124Z","shell.execute_reply":"2024-12-05T07:40:08.637336Z"}},"outputs":[{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_23/2085385683.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"/kaggle/working/ElectraForSequenceClassification.pt\"))\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.3245, device='cuda:0')\n0.874\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}